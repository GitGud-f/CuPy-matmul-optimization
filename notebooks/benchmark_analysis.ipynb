{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68eba482",
   "metadata": {},
   "source": [
    " ## Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd24c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Directories created: src, kernels, notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 1. Create Directories\n",
    "os.makedirs('src', exist_ok=True)\n",
    "os.makedirs('kernels', exist_ok=True)\n",
    "os.makedirs('notebooks', exist_ok=True)\n",
    "\n",
    "print(\"✅ Directories created: src, kernels, notebooks\")\n",
    "\n",
    "with open('src/__init__.py', 'w') as f:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35361489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils.py\n",
    "import time\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "def generate_matrices(n, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Generates two random N*N matrices A and B.\n",
    "    Using float32 is standard for GPU programming (single precision).\n",
    "    Args:\n",
    "        n: Size of the matrices (N x N).\n",
    "        dtype: Data type of the matrices (default: np.float32).\n",
    "    Returns: \n",
    "        Two N x N matrices A and B.\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.random.rand(n, n).astype(dtype)\n",
    "    B = np.random.rand(n, n).astype(dtype)\n",
    "    return A, B\n",
    "\n",
    "def check_correctness(target, reference, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    Compares two matrices using NumPy's allclose.\n",
    "    Args:\n",
    "        target: The matrix to test.\n",
    "        reference: The reference matrix.\n",
    "        tolerance: The tolerance for comparison (default: 1e-4).\n",
    "    Returns:\n",
    "        True if matrices are close within the given tolerance, False otherwise.\n",
    "    \"\"\"\n",
    "    if hasattr(target, 'get'): \n",
    "        target = target.get()\n",
    "    if hasattr(reference, 'get'): \n",
    "        reference = reference.get()\n",
    "    try:\n",
    "        np.testing.assert_allclose(target, reference, atol=tolerance, rtol=tolerance)\n",
    "        return True\n",
    "    except AssertionError:\n",
    "        return False\n",
    "\n",
    "def benchmark_function(func, name, *args):\n",
    "    \"\"\"\n",
    "    Benchmarks the execution time of a given function.\n",
    "    Args:\n",
    "        func: The function to benchmark.\n",
    "        name: Name of the function (for reporting).\n",
    "        *args: Arguments to pass to the function.\n",
    "    Returns:\n",
    "        A tuple containing the result of the function and the execution time in milliseconds.\n",
    "    \"\"\"\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    result = func(*args)\n",
    "    \n",
    "    cp.cuda.Device(0).synchronize()\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    execution_time_ms = (end_time - start_time) * 1000\n",
    "    print(f\"[{name}] Execution Time: {execution_time_ms:.4f} ms\")\n",
    "    return result, execution_time_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b591cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cpu_baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cpu_baseline.py\n",
    "import numpy as np\n",
    "\n",
    "def cpu_matmul(A, B):\n",
    "    \"\"\"\n",
    "    Standard Matrix Multiplication using Triple Nested Loops.\n",
    "    C[i][j] = sum(A[i][k] * B[k][j])\n",
    "    Args:\n",
    "        A: First input matrix.\n",
    "        B: Second input matrix.\n",
    "    Returns:\n",
    "        The resulting matrix after multiplication C = A * B.\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    \n",
    "    rows_A, cols_A = A.shape\n",
    "    rows_B, cols_B = B.shape\n",
    "    \n",
    "    if cols_A != rows_B:\n",
    "        raise ValueError(\"Cannot multiply: Dimensions do not match.\")\n",
    "        \n",
    "    C = np.zeros((rows_A, cols_B), dtype=A.dtype)\n",
    "    \n",
    "    for i in range(rows_A):          \n",
    "        for j in range(cols_B):      \n",
    "            total = 0\n",
    "            for k in range(cols_A):  \n",
    "                total += A[i, k] * B[k, j]\n",
    "            C[i, j] = total\n",
    "            \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01297d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/gpu_ops.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/gpu_ops.py\n",
    "\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def transfer_to_gpu(A_host: np.ndarray, B_host: np.ndarray) -> tuple:\n",
    "    \n",
    "    \"\"\"\n",
    "    Transfers numpy arrays from Host (CPU) to Device (GPU).\n",
    "    Args:\n",
    "        A_host: First input matrix on host (CPU).\n",
    "        B_host: Second input matrix on host (CPU).\n",
    "    Returns:\n",
    "        Two matrices A and B on device (GPU).\n",
    "    \"\"\"\n",
    "    A_gpu = cp.asarray(A_host)\n",
    "    B_gpu = cp.asarray(B_host)\n",
    "    return A_gpu, B_gpu\n",
    "\n",
    "def cupy_matmul_library(A_gpu: cp.ndarray, B_gpu: cp.ndarray) -> cp.ndarray:\n",
    "    \"\"\"\n",
    "    Performs Matrix Multiplication using CuPy's optimized library.\n",
    "    Args:\n",
    "        A_gpu: First input matrix on device (GPU).\n",
    "        B_gpu: Second input matrix on device (GPU).\n",
    "    Returns:    \n",
    "        The resulting matrix after multiplication C = A * B on device (GPU).\n",
    "    \"\"\"\n",
    "    return cp.matmul(A_gpu, B_gpu)\n",
    "\n",
    "def run_custom_kernel(kernel_source: str, function_name: str, grid: tuple, block: tuple, args: tuple):\n",
    "    \"\"\"\n",
    "    Compiles and executes a raw CUDA kernel.\n",
    "    Args:\n",
    "        kernel_source: The source code of the CUDA kernel as a string.\n",
    "        function_name: The name of the kernel function to execute.\n",
    "        grid: The grid dimensions for kernel launch.\n",
    "        block: The block dimensions for kernel launch.\n",
    "        args: The arguments to pass to the kernel.\n",
    "    \"\"\"\n",
    "    module = cp.RawModule(code=kernel_source)\n",
    "    kernel = module.get_function(function_name)\n",
    "    kernel(grid, block, args)\n",
    "    \n",
    "def run_naive_kernel(A_gpu, B_gpu, N, block_size=(16, 16)):\n",
    "    \"\"\"\n",
    "    Runs the naive matrix multiplication kernel.\n",
    "    Args:\n",
    "        A_gpu: First input matrix on device (GPU).\n",
    "        B_gpu: Second input matrix on device (GPU).\n",
    "        N: Size of the matrices (N x N).\n",
    "        block_size: The block dimensions for kernel launch (default: (16, 16)).\n",
    "    Returns:    \n",
    "        The resulting matrix after multiplication C = A * B on device (GPU).\n",
    "    \"\"\"\n",
    "    with open('kernels/matmul.cu', 'r') as f:\n",
    "        kernel_code = f.read()\n",
    "    \n",
    "    kernel = cp.RawKernel(kernel_code, 'matmul_kernel')\n",
    "    \n",
    "    C_gpu = cp.zeros((N, N), dtype=cp.float32)\n",
    "    \n",
    "    grid_x = (N + block_size[0] - 1) // block_size[0]\n",
    "    grid_y = (N + block_size[1] - 1) // block_size[1]\n",
    "    grid_dim = (grid_x, grid_y)\n",
    "    \n",
    "    kernel(grid_dim, block_size, (A_gpu, B_gpu, C_gpu, cp.int32(N)))\n",
    "    \n",
    "    return C_gpu\n",
    "\n",
    "def run_tiled_kernel_dynamic(A_gpu, B_gpu, N, tile_size=16):\n",
    "    \"\"\"\n",
    "    Compiles and runs the Tiled Kernel with a specific Tile Size.\n",
    "    Injects the #define macro dynamically.\n",
    "    Args:\n",
    "        A_gpu: First input matrix on device (GPU).\n",
    "        B_gpu: Second input matrix on device (GPU).\n",
    "        N: Size of the matrices (N x N).\n",
    "        tile_size: The tile size to define in the kernel (default: 16).\n",
    "    Returns:    \n",
    "        The resulting matrix after multiplication C = A * B on device (GPU).\n",
    "    \"\"\"\n",
    "    with open('kernels/tiled_matmul.cu', 'r') as f:\n",
    "        raw_code = f.read()\n",
    "    \n",
    "    augmented_code = f\"#define TILE_WIDTH {tile_size}\\n\" + raw_code\n",
    "    \n",
    "    kernel = cp.RawKernel(augmented_code, 'tiled_matmul_kernel')\n",
    "    \n",
    "    C_gpu = cp.zeros((N, N), dtype=cp.float32)\n",
    "    \n",
    "    block_dim = (tile_size, tile_size)\n",
    "    grid_x = (N + tile_size - 1) // tile_size\n",
    "    grid_y = (N + tile_size - 1) // tile_size\n",
    "    \n",
    "    kernel((grid_x, grid_y), block_dim, (A_gpu, B_gpu, C_gpu, cp.int32(N)))\n",
    "    \n",
    "    return C_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a05ec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kernels/matmul.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile kernels/matmul.cu\n",
    "extern \"C\" {\n",
    "    __global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n",
    "        \n",
    "\n",
    "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "        if (row < N && col < N) {\n",
    "            \n",
    "            float sum = 0.0f;\n",
    "            \n",
    "            for (int k = 0; k < N; k++) {\n",
    "                float a = A[row * N + k];\n",
    "                float b = B[k * N + col];\n",
    "                sum += a * b;\n",
    "            }\n",
    "\n",
    "            C[row * N + col] = sum;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bac29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kernels/tiled_matmul.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile kernels/tiled_matmul.cu\n",
    "extern \"C\" {\n",
    "    // Note: TILE_WIDTH is NOT defined here. \n",
    "    // It will be injected by the Python script before compilation.\n",
    "    // This allows us to benchmark TILE_WIDTH = 16 vs 32 without changing this file.\n",
    "\n",
    "    __global__ void tiled_matmul_kernel(const float* A, const float* B, float* C, int N) {\n",
    "        \n",
    "        // 1. Allocate Shared Memory\n",
    "        // The size depends on TILE_WIDTH (injected via macro)\n",
    "        __shared__ float As[TILE_WIDTH][TILE_WIDTH];\n",
    "        __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];\n",
    "\n",
    "        // 2. Setup Indices\n",
    "        int bx = blockIdx.x;  int by = blockIdx.y;\n",
    "        int tx = threadIdx.x; int ty = threadIdx.y;\n",
    "\n",
    "        // Row and Col of the element C we are computing\n",
    "        int row = by * TILE_WIDTH + ty;\n",
    "        int col = bx * TILE_WIDTH + tx;\n",
    "\n",
    "        float value = 0.0f;\n",
    "\n",
    "        // 3. Loop over tiles\n",
    "        // We move the tile window across the matrices\n",
    "        int num_phases = (N + TILE_WIDTH - 1) / TILE_WIDTH;\n",
    "\n",
    "        for (int m = 0; m < num_phases; ++m) {\n",
    "\n",
    "            // --- Loading Phase ---\n",
    "            // Each thread loads ONE element into shared memory\n",
    "            \n",
    "            // Load A[row][m*TILE + tx]\n",
    "            int col_A = m * TILE_WIDTH + tx;\n",
    "            if (row < N && col_A < N)\n",
    "                As[ty][tx] = A[row * N + col_A];\n",
    "            else\n",
    "                As[ty][tx] = 0.0f;\n",
    "\n",
    "            // Load B[m*TILE + ty][col]\n",
    "            int row_B = m * TILE_WIDTH + ty;\n",
    "            if (row_B < N && col < N)\n",
    "                Bs[ty][tx] = B[row_B * N + col];\n",
    "            else\n",
    "                Bs[ty][tx] = 0.0f;\n",
    "\n",
    "            // Wait for all threads to finish loading\n",
    "            __syncthreads();\n",
    "\n",
    "            // --- Computation Phase ---\n",
    "            // Compute partial dot product from Shared Memory (Fast Access)\n",
    "            for (int k = 0; k < TILE_WIDTH; ++k) {\n",
    "                value += As[ty][k] * Bs[k][tx];\n",
    "            }\n",
    "\n",
    "            // Wait for all threads to finish computing before overwriting Shared Mem\n",
    "            __syncthreads();\n",
    "        }\n",
    "\n",
    "        // 4. Write Result\n",
    "        if (row < N && col < N) {\n",
    "            C[row * N + col] = value;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29317c",
   "metadata": {},
   "source": [
    "## CPU Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33ab143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully.\n",
      "Part 1: CPU Baseline Benchmark (N=512)\n",
      "[CPU Naive] Execution Time: 54421.9207 ms\n",
      "PASS: CPU implementation matches NumPy reference.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from src.utils import generate_matrices, check_correctness, benchmark_function\n",
    "from src.cpu_baseline import cpu_matmul\n",
    "\n",
    "print(\"Modules imported successfully.\")\n",
    "\n",
    "\n",
    "N = 512\n",
    "print(f\"Part 1: CPU Baseline Benchmark (N={N})\")\n",
    "\n",
    "A_host, B_host = generate_matrices(N)\n",
    "\n",
    "\n",
    "C_cpu, time_cpu = benchmark_function(cpu_matmul, \"CPU Naive\", A_host, B_host)\n",
    "\n",
    "C_ref = np.dot(A_host, B_host)\n",
    "\n",
    "if check_correctness(C_cpu, C_ref):\n",
    "    print(\"PASS: CPU implementation matches NumPy reference.\")\n",
    "else:\n",
    "    print(\"FAIL: CPU implementation is incorrect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c882e3",
   "metadata": {},
   "source": [
    "## GPU using CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce70e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2: CuPy (GPU) Implementation\n",
      "\n",
      "Experiment A: Small Matrix (N=512)\n",
      "Transferring data to GPU... Done.\n",
      "Warming up GPU... Done.\n",
      "[CuPy Library] Execution Time: 0.3781 ms\n",
      "PASS: CuPy result matches Reference.\n",
      "Speedup vs CPU: 143927.26x\n",
      "\n",
      "Experiment B: Large Matrix (N=2000)\n",
      "[CuPy (N=2000)] Execution Time: 6.4436 ms\n",
      "Note: A CPU naive loop for N=2000 would take hours.\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from src.gpu_ops import transfer_to_gpu, cupy_matmul_library\n",
    "\n",
    "print(\"Part 2: CuPy (GPU) Implementation\")\n",
    "\n",
    "print(f\"\\nExperiment A: Small Matrix (N={N})\")\n",
    "\n",
    "\n",
    "print(\"Transferring data to GPU...\", end=\" \")\n",
    "A_gpu, B_gpu = transfer_to_gpu(A_host, B_host) \n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Warming up GPU...\", end=\" \")\n",
    "cupy_matmul_library(A_gpu, B_gpu)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "print(\"Done.\")\n",
    "\n",
    "C_gpu, time_gpu = benchmark_function(cupy_matmul_library, \"CuPy Library\", A_gpu, B_gpu)\n",
    "\n",
    "\n",
    "if check_correctness(C_gpu, C_ref):\n",
    "    print(\"PASS: CuPy result matches Reference.\")\n",
    "    print(f\"Speedup vs CPU: {time_cpu / time_gpu:.2f}x\")\n",
    "else:\n",
    "    print(\"FAIL: CuPy result incorrect.\")\n",
    "\n",
    "\n",
    "N_large = 2000\n",
    "print(f\"\\nExperiment B: Large Matrix (N={N_large})\")\n",
    "\n",
    "A_large, B_large = generate_matrices(N_large)\n",
    "A_large_gpu, B_large_gpu = transfer_to_gpu(A_large, B_large)\n",
    "\n",
    "C_large_gpu, time_large_gpu = benchmark_function(cupy_matmul_library, f\"CuPy (N={N_large})\", A_large_gpu, B_large_gpu)\n",
    "\n",
    "print(f\"Note: A CPU naive loop for N={N_large} would take hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc89d34",
   "metadata": {},
   "source": [
    "## Custom Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "468c5dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3: Custom Kernel Benchmark\n",
      "Matrix Size: 2000x2000\n",
      "\n",
      "--- Testing Block Size: (8, 8) ---\n",
      "[Naive Kernel (8, 8)] Execution Time: 116.2583 ms\n",
      "Result Correct\n",
      "\n",
      "--- Testing Block Size: (16, 16) ---\n",
      "[Naive Kernel (16, 16)] Execution Time: 59.3721 ms\n",
      "Result Correct\n",
      "\n",
      "--- Testing Block Size: (32, 32) ---\n",
      "[Naive Kernel (32, 32)] Execution Time: 43.8809 ms\n",
      "Result Correct\n",
      "\n",
      "--- Comparison ---\n",
      "[CuPy Library] Execution Time: 5.3457 ms\n",
      "Library is 8.21x faster than the Naive Kernel.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from src.gpu_ops import run_naive_kernel, transfer_to_gpu\n",
    "\n",
    "print(\"Part 3: Custom Kernel Benchmark\")\n",
    "\n",
    "N = 2000 \n",
    "print(f\"Matrix Size: {N}x{N}\")\n",
    "\n",
    "A_host, B_host = generate_matrices(N)\n",
    "A_gpu, B_gpu = transfer_to_gpu(A_host, B_host)\n",
    "\n",
    "block_sizes = [(8, 8), (16, 16), (32, 32)]\n",
    "\n",
    "for bs in block_sizes:\n",
    "    print(f\"\\n--- Testing Block Size: {bs} ---\")\n",
    "    \n",
    "    func_to_test = lambda: run_naive_kernel(A_gpu, B_gpu, N, block_size=bs)\n",
    "    \n",
    "\n",
    "    C_custom, time_custom = benchmark_function(func_to_test, f\"Naive Kernel {bs}\")\n",
    "    \n",
    "    C_ref_gpu = cupy_matmul_library(A_gpu, B_gpu)\n",
    "    if check_correctness(C_custom, C_ref_gpu, tolerance=1e-3):\n",
    "        print(\"Result Correct\")\n",
    "    else:\n",
    "        print(\"Result Incorrect\")\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "\n",
    "_, time_lib = benchmark_function(lambda: cupy_matmul_library(A_gpu, B_gpu), \"CuPy Library\")\n",
    "print(f\"Library is {time_custom/time_lib:.2f}x faster than the Naive Kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aec7d9",
   "metadata": {},
   "source": [
    "## Memory Tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "380ca8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 4: Tiling Optimization (Dynamic)\n",
      "Matrix Size: 2000x2000\n",
      "\n",
      "--- Baseline: Naive ---\n",
      "[Naive (16x16)] Execution Time: 59.0406 ms\n",
      "\n",
      "--- Tiling Experiments ---\n",
      "[Tiled (16x16)] Execution Time: 41.3995 ms\n",
      "   ✅ Tiled (16x16) Validated\n",
      "[Tiled (32x32)] Execution Time: 40.3589 ms\n",
      "   ✅ Tiled (32x32) Validated\n",
      "\n",
      "=== Final Analysis ===\n",
      "Best Configuration: Tiled 32x32\n",
      "Speedup (Tiled vs Naive): 1.46x\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import src.gpu_ops\n",
    "importlib.reload(src.gpu_ops)\n",
    "from src.gpu_ops import run_tiled_kernel_dynamic, run_naive_kernel, transfer_to_gpu, cupy_matmul_library\n",
    "from src.utils import benchmark_function, check_correctness, generate_matrices\n",
    "\n",
    "print(\"Part 4: Tiling Optimization (Dynamic)\")\n",
    "N = 2000\n",
    "print(f\"Matrix Size: {N}x{N}\")\n",
    "\n",
    "A_host, B_host = generate_matrices(N)\n",
    "A_gpu, B_gpu = transfer_to_gpu(A_host, B_host)\n",
    "\n",
    "print(\"\\n--- Baseline: Naive ---\")\n",
    "_, time_naive = benchmark_function(\n",
    "    lambda: run_naive_kernel(A_gpu, B_gpu, N, block_size=(16,16)), \n",
    "    \"Naive (16x16)\"\n",
    ")\n",
    "\n",
    "tile_configs = [16, 32]\n",
    "results = {}\n",
    "\n",
    "print(\"\\n--- Tiling Experiments ---\")\n",
    "for size in tile_configs:\n",
    "    name = f\"Tiled ({size}x{size})\"\n",
    "    \n",
    "    C_tiled, time_tiled = benchmark_function(\n",
    "        lambda: run_tiled_kernel_dynamic(A_gpu, B_gpu, N, tile_size=size), \n",
    "        name\n",
    "    )\n",
    "    \n",
    "    C_ref = cupy_matmul_library(A_gpu, B_gpu)\n",
    "    if check_correctness(C_tiled, C_ref, tolerance=1e-3):\n",
    "        print(f\"   ✅ {name} Validated\")\n",
    "        results[size] = time_tiled\n",
    "    else:\n",
    "        print(f\"   ❌ {name} FAILED\")\n",
    "\n",
    "print(\"\\n=== Final Analysis ===\")\n",
    "best_tile = min(results, key=results.get)\n",
    "print(f\"Best Configuration: Tiled {best_tile}x{best_tile}\")\n",
    "print(f\"Speedup (Tiled vs Naive): {time_naive / results[best_tile]:.2f}x\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
