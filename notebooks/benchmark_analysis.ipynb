{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68eba482",
   "metadata": {},
   "source": [
    " ## Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd24c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Directories created: src, kernels, notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('src', exist_ok=True)\n",
    "os.makedirs('kernels', exist_ok=True)\n",
    "os.makedirs('notebooks', exist_ok=True)\n",
    "\n",
    "print(\"✅ Directories created: src, kernels, notebooks\")\n",
    "\n",
    "with open('src/__init__.py', 'w') as f:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35361489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils.py\n",
    "from typing import Tuple\n",
    "import time\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "def generate_matrices(n: int, dtype=np.float32) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates two random N*N matrices A and B.\n",
    "    Using float32 is standard for GPU programming (single precision).\n",
    "    Args:\n",
    "        n: Size of the matrices (N x N).\n",
    "        dtype: Data type of the matrices (default: np.float32).\n",
    "    Returns: \n",
    "        Two N x N matrices A and B.\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.random.rand(n, n).astype(dtype)\n",
    "    B = np.random.rand(n, n).astype(dtype)\n",
    "    return A, B\n",
    "\n",
    "def check_correctness(target: np.ndarray, reference: np.ndarray, tolerance: float = 1e-4) -> bool:\n",
    "    \"\"\"\n",
    "    Compares two matrices using NumPy's allclose.\n",
    "    Args:\n",
    "        target: The matrix to test.\n",
    "        reference: The reference matrix.\n",
    "        tolerance: The tolerance for comparison (default: 1e-4).\n",
    "    Returns:\n",
    "        True if matrices are close within the given tolerance, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(target, 'get'): \n",
    "        target = target.get()\n",
    "    if hasattr(reference, 'get'): \n",
    "        reference = reference.get()\n",
    "    try:\n",
    "        np.testing.assert_allclose(target, reference, atol=tolerance, rtol=tolerance)\n",
    "        return True\n",
    "    except AssertionError:\n",
    "        return False\n",
    "\n",
    "def benchmark_function(func, name: str, *args, n_iter: int = 5) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Benchmarks a given function by running it multiple times and measuring execution time.\n",
    "    Args:\n",
    "        func: The function to benchmark.\n",
    "        name: Name of the benchmark (for reporting).\n",
    "        *args: Arguments to pass to the function.\n",
    "        n_iter: Number of iterations to run (default: 5).\n",
    "    Returns:\n",
    "        A tuple containing the result of the function and the average execution time in milliseconds.\n",
    "    \"\"\"\n",
    "    func(*args) # warm-up\n",
    "    cp.cuda.Device(0).synchronize()\n",
    "    \n",
    "    timings = []\n",
    "    result = None\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        cp.cuda.Device(0).synchronize()\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args) \n",
    "        \n",
    "        cp.cuda.Device(0).synchronize()\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        timings.append((end_time - start_time) * 1000)\n",
    "\n",
    "    avg_time = np.mean(timings)\n",
    "    std_dev = np.std(timings)\n",
    "    \n",
    "    print(f\"[{name}] Avg Time: {avg_time:.4f} ms (±{std_dev:.2f} ms) | Runs: {n_iter}\")\n",
    "    \n",
    "    return result, avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b591cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cpu_baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cpu_baseline.py\n",
    "import numpy as np\n",
    "\n",
    "def cpu_matmul(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Standard Matrix Multiplication using Triple Nested Loops.\n",
    "    C[i][j] = sum(A[i][k] * B[k][j])\n",
    "    Args:\n",
    "        A: First input matrix.\n",
    "        B: Second input matrix.\n",
    "    Returns:\n",
    "        The resulting matrix after multiplication C = A * B.\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "    \n",
    "    rows_A, cols_A = A.shape\n",
    "    rows_B, cols_B = B.shape\n",
    "    \n",
    "    if cols_A != rows_B:\n",
    "        raise ValueError(\"Cannot multiply: Dimensions do not match.\")\n",
    "        \n",
    "    C = np.zeros((rows_A, cols_B), dtype=A.dtype)\n",
    "    \n",
    "    for i in range(rows_A):          \n",
    "        for j in range(cols_B):      \n",
    "            total = 0\n",
    "            for k in range(cols_A):  \n",
    "                total += A[i, k] * B[k, j]\n",
    "            C[i, j] = total\n",
    "            \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01297d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/gpu_ops.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/gpu_ops.py\n",
    "\n",
    "\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def transfer_to_gpu(A_host: np.ndarray, B_host: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Transfers numpy arrays from Host (CPU) to Device (GPU).\n",
    "    Args:\n",
    "        A_host: First input matrix on host (CPU).\n",
    "        B_host: Second input matrix on host (CPU).\n",
    "    Returns:\n",
    "        Two matrices A and B on device (GPU).\n",
    "    \"\"\"\n",
    "    A_gpu = cp.asarray(A_host)\n",
    "    B_gpu = cp.asarray(B_host)\n",
    "    return A_gpu, B_gpu\n",
    "\n",
    "def cupy_matmul_library(A_gpu: cp.ndarray, B_gpu: cp.ndarray) -> cp.ndarray:\n",
    "    \"\"\"\n",
    "    Performs Matrix Multiplication using CuPy's optimized library.\n",
    "    Args:\n",
    "        A_gpu: First input matrix on device (GPU).\n",
    "        B_gpu: Second input matrix on device (GPU).\n",
    "    Returns:    \n",
    "        The resulting matrix after multiplication C = A * B on device (GPU).\n",
    "    \"\"\"\n",
    "    return cp.matmul(A_gpu, B_gpu)\n",
    "\n",
    "def run_custom_kernel(A_gpu: cp.ndarray, B_gpu: cp.ndarray, N: int, block_size=(16, 16)) -> cp.ndarray:\n",
    "    \"\"\"\n",
    "    Runs a custom naive matrix multiplication kernel.\n",
    "    Args:\n",
    "        A_gpu: First input matrix on device (GPU).\n",
    "        B_gpu: Second input matrix on device (GPU).\n",
    "        N: Size of the NxN matrices.    \n",
    "        block_size: The block size to use in the kernel launch (default: (16,16)).\n",
    "    Returns:    \n",
    "        The resulting matrix C = A * B on device (GPU).\n",
    "    \"\"\"\n",
    "\n",
    "    with open('kernels/matmul.cu', 'r') as f:\n",
    "        kernel_code = f.read()\n",
    "    \n",
    "    kernel = cp.RawKernel(kernel_code, 'matmul_kernel')\n",
    "    \n",
    "    C_gpu = cp.zeros((N, N), dtype=cp.float32)\n",
    "    \n",
    "    grid_x = (N + block_size[0] - 1) // block_size[0]\n",
    "    grid_y = (N + block_size[1] - 1) // block_size[1]\n",
    "    grid_dim = (grid_x, grid_y)\n",
    "    \n",
    "    kernel(grid_dim, block_size, (A_gpu, B_gpu, C_gpu, cp.int32(N)))\n",
    "    \n",
    "    return C_gpu\n",
    "    \n",
    "def run_tiled_kernel_dynamic(A_gpu: cp.ndarray, B_gpu: cp.ndarray, N: int, tile_size: int =16) -> cp.ndarray:\n",
    "    \"\"\"\n",
    "    Runs a tiled matrix multiplication kernel with dynamic tile size.\n",
    "    Args:\n",
    "        A_gpu: First input matrix on device (GPU).\n",
    "        B_gpu: Second input matrix on device (GPU).\n",
    "        N: Size of the NxN matrices.\n",
    "        tile_size: The tile size to use in the kernel.\n",
    "    Returns:    \n",
    "        The resulting matrix C = A * B on device (GPU).\n",
    "    \"\"\"\n",
    "\n",
    "    with open('kernels/tiled_matmul.cu', 'r') as f:\n",
    "        raw_code = f.read()\n",
    "    \n",
    "\n",
    "    augmented_code = f\"#define TILE_WIDTH {tile_size}\\n\" + raw_code\n",
    "    \n",
    "    kernel = cp.RawKernel(augmented_code, 'tiled_matmul_kernel')\n",
    "    \n",
    "    C_gpu = cp.zeros((N, N), dtype=cp.float32)\n",
    "    \n",
    "\n",
    "    block_dim = (tile_size, tile_size)\n",
    "    grid_x = (N + tile_size - 1) // tile_size\n",
    "    grid_y = (N + tile_size - 1) // tile_size\n",
    "    \n",
    "    kernel((grid_x, grid_y), block_dim, (A_gpu, B_gpu, C_gpu, cp.int32(N)))\n",
    "    \n",
    "    return C_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a05ec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kernels/matmul.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile kernels/matmul.cu\n",
    "extern \"C\" {\n",
    "    __global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n",
    "        \n",
    "\n",
    "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "        if (row < N && col < N) {\n",
    "            \n",
    "            float sum = 0.0f;\n",
    "            \n",
    "            for (int k = 0; k < N; k++) {\n",
    "                float a = A[row * N + k];\n",
    "                float b = B[k * N + col];\n",
    "                sum += a * b;\n",
    "            }\n",
    "\n",
    "            C[row * N + col] = sum;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bac29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kernels/tiled_matmul.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile kernels/tiled_matmul.cu\n",
    "extern \"C\" {\n",
    "\n",
    "    __global__ void tiled_matmul_kernel(const float* A, const float* B, float* C, int N) {\n",
    "        \n",
    "        __shared__ float As[TILE_WIDTH][TILE_WIDTH];\n",
    "        __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];\n",
    "\n",
    "        int bx = blockIdx.x;  int by = blockIdx.y;\n",
    "        int tx = threadIdx.x; int ty = threadIdx.y;\n",
    "\n",
    "        int row = by * TILE_WIDTH + ty;\n",
    "        int col = bx * TILE_WIDTH + tx;\n",
    "\n",
    "        float value = 0.0f;\n",
    "\n",
    "        int num_phases = (N + TILE_WIDTH - 1) / TILE_WIDTH;\n",
    "\n",
    "        for (int m = 0; m < num_phases; ++m) {\n",
    "\n",
    "\n",
    "            \n",
    "            int col_A = m * TILE_WIDTH + tx;\n",
    "            if (row < N && col_A < N)\n",
    "                As[ty][tx] = A[row * N + col_A];\n",
    "            else\n",
    "                As[ty][tx] = 0.0f;\n",
    "\n",
    "            int row_B = m * TILE_WIDTH + ty;\n",
    "            if (row_B < N && col < N)\n",
    "                Bs[ty][tx] = B[row_B * N + col];\n",
    "            else\n",
    "                Bs[ty][tx] = 0.0f;\n",
    "\n",
    "            __syncthreads();\n",
    "\n",
    "\n",
    "            for (int k = 0; k < TILE_WIDTH; ++k) {\n",
    "                value += As[ty][k] * Bs[k][tx];\n",
    "            }\n",
    "\n",
    "            __syncthreads();\n",
    "        }\n",
    "\n",
    "        if (row < N && col < N) {\n",
    "            C[row * N + col] = value;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbe50e",
   "metadata": {},
   "source": [
    "## File Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcfb5cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "from src.utils import generate_matrices, check_correctness, benchmark_function\n",
    "from src.cpu_baseline import cpu_matmul\n",
    "from src.gpu_ops import transfer_to_gpu, cupy_matmul_library, run_custom_kernel, run_tiled_kernel_dynamic\n",
    "\n",
    "print(\"Modules imported successfully.\")\n",
    "N = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29317c",
   "metadata": {},
   "source": [
    "## CPU Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c33ab143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1: CPU Baseline Benchmark (N=256)\n",
      "[CPU Naive] Avg Time: 6592.3083 ms (±428.07 ms) | Runs: 5\n",
      "PASS: CPU implementation matches NumPy reference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Part 1: CPU Baseline Benchmark (N={N})\")\n",
    "\n",
    "A_host, B_host = generate_matrices(N)\n",
    "\n",
    "\n",
    "C_cpu, time_cpu = benchmark_function(cpu_matmul, \"CPU Naive\", A_host, B_host, n_iter=5)\n",
    "\n",
    "C_ref = np.dot(A_host, B_host)\n",
    "\n",
    "if check_correctness(C_cpu, C_ref):\n",
    "    print(\"PASS: CPU implementation matches NumPy reference.\")\n",
    "else:\n",
    "    print(\"FAIL: CPU implementation is incorrect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c882e3",
   "metadata": {},
   "source": [
    "## GPU using CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce70e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2: CuPy (GPU) Implementation\n",
      "\n",
      "Experiment A: Small Matrix (N=256)\n",
      "Transferring data to GPU... Done.\n",
      "Warming up GPU... Done.\n",
      "[CuPy Library] Avg Time: 0.1976 ms (±0.06 ms) | Runs: 5\n",
      "PASS: CuPy result matches Reference.\n",
      "Speedup vs CPU: 33355.74x\n",
      "\n",
      "Experiment B: Large Matrix (N=2000)\n",
      "[CuPy (N=2000)] Avg Time: 6.0550 ms (±0.06 ms) | Runs: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 2: CuPy (GPU) Implementation\")\n",
    "\n",
    "print(f\"\\nExperiment A: Small Matrix (N={N})\")\n",
    "\n",
    "\n",
    "print(\"Transferring data to GPU...\", end=\" \")\n",
    "A_gpu, B_gpu = transfer_to_gpu(A_host, B_host) \n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Warming up GPU...\", end=\" \")\n",
    "cupy_matmul_library(A_gpu, B_gpu)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "print(\"Done.\")\n",
    "\n",
    "C_gpu, time_gpu = benchmark_function(cupy_matmul_library, \"CuPy Library\", A_gpu, B_gpu, n_iter=5)\n",
    "\n",
    "\n",
    "if check_correctness(C_gpu, C_ref):\n",
    "    print(\"PASS: CuPy result matches Reference.\")\n",
    "    print(f\"Speedup vs CPU: {time_cpu / time_gpu:.2f}x\")\n",
    "else:\n",
    "    print(\"FAIL: CuPy result incorrect.\")\n",
    "\n",
    "\n",
    "N_large = 2000\n",
    "print(f\"\\nExperiment B: Large Matrix (N={N_large})\")\n",
    "\n",
    "A_large, B_large = generate_matrices(N_large)\n",
    "A_large_gpu, B_large_gpu = transfer_to_gpu(A_large, B_large)\n",
    "\n",
    "C_large_gpu, time_large_gpu = benchmark_function(cupy_matmul_library, f\"CuPy (N={N_large})\", A_large_gpu, B_large_gpu, n_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc89d34",
   "metadata": {},
   "source": [
    "## Custom Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "468c5dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3: Custom Kernel Benchmark\n",
      "Matrix Size: 2000x2000\n",
      "\n",
      "--- Testing Block Size: (8, 8) ---\n",
      "[Naive Kernel (8, 8)] Avg Time: 67.3351 ms (±3.37 ms) | Runs: 5\n",
      "Result Correct\n",
      "\n",
      "--- Testing Block Size: (16, 16) ---\n",
      "[Naive Kernel (16, 16)] Avg Time: 40.8900 ms (±3.92 ms) | Runs: 5\n",
      "Result Correct\n",
      "\n",
      "--- Testing Block Size: (32, 32) ---\n",
      "[Naive Kernel (32, 32)] Avg Time: 32.8921 ms (±3.97 ms) | Runs: 5\n",
      "Result Correct\n",
      "\n",
      "--- Comparison ---\n",
      "[CuPy Library] Avg Time: 2.5780 ms (±0.02 ms) | Runs: 5\n",
      "Library is 12.76x faster than the Naive Kernel.\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 3: Custom Kernel Benchmark\")\n",
    "\n",
    "N = 2000 \n",
    "print(f\"Matrix Size: {N}x{N}\")\n",
    "\n",
    "A_host, B_host = generate_matrices(N)\n",
    "A_gpu, B_gpu = transfer_to_gpu(A_host, B_host)\n",
    "\n",
    "block_sizes = [(8, 8), (16, 16), (32, 32)]\n",
    "\n",
    "for bs in block_sizes:\n",
    "    print(f\"\\n--- Testing Block Size: {bs} ---\")\n",
    "    \n",
    "    func = lambda: run_custom_kernel(A_gpu, B_gpu, N, block_size=bs)\n",
    "    C_custom, time_custom = benchmark_function(func, f\"Naive Kernel {bs}\", n_iter=5)\n",
    "    \n",
    "    C_ref_gpu = cupy_matmul_library(A_gpu, B_gpu)\n",
    "    if check_correctness(C_custom, C_ref_gpu, tolerance=1e-3):\n",
    "        print(\"Result Correct\")\n",
    "    else:\n",
    "        print(\"Result Incorrect\")\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "\n",
    "_, time_lib = benchmark_function(cupy_matmul_library, \"CuPy Library\", A_gpu, B_gpu, n_iter=5)\n",
    "print(f\"Library is {time_custom/time_lib:.2f}x faster than the Naive Kernel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aec7d9",
   "metadata": {},
   "source": [
    "## Memory Tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "380ca8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 4: Tiling Optimization (Dynamic)\n",
      "Matrix Size: 2000x2000\n",
      "\n",
      "--- Baseline: Naive ---\n",
      "[Naive (16x16)] Avg Time: 40.0066 ms (±4.56 ms) | Runs: 5\n",
      "\n",
      "--- Tiling Experiments ---\n",
      "[Tiled (16x16)] Avg Time: 27.3159 ms (±0.11 ms) | Runs: 5\n",
      "Tiled (16x16) Validated\n",
      "[Tiled (32x32)] Avg Time: 20.8915 ms (±2.33 ms) | Runs: 5\n",
      "Tiled (32x32) Validated\n",
      "\n",
      "=== Final Analysis ===\n",
      "Best Configuration: Tiled 32x32\n",
      "Speedup (Tiled vs Naive): 1.91x\n"
     ]
    }
   ],
   "source": [
    "print(\"Part 4: Tiling Optimization (Dynamic)\")\n",
    "N = 2000\n",
    "print(f\"Matrix Size: {N}x{N}\")\n",
    "\n",
    "A_host, B_host = generate_matrices(N)\n",
    "A_gpu, B_gpu = transfer_to_gpu(A_host, B_host)\n",
    "\n",
    "print(\"\\n--- Baseline: Naive ---\")\n",
    "_, time_naive = benchmark_function(\n",
    "    lambda: run_custom_kernel(A_gpu, B_gpu, N, block_size=(16,16)), \n",
    "    \"Naive (16x16)\",\n",
    "    n_iter=5\n",
    ")\n",
    "\n",
    "tile_configs = [16, 32]\n",
    "results = {}\n",
    "\n",
    "print(\"\\n--- Tiling Experiments ---\")\n",
    "for size in tile_configs:\n",
    "    name = f\"Tiled ({size}x{size})\"\n",
    "    \n",
    "    C_tiled, time_tiled = benchmark_function(\n",
    "        lambda: run_tiled_kernel_dynamic(A_gpu, B_gpu, N, tile_size=size), \n",
    "        name,\n",
    "        n_iter=5\n",
    "    )\n",
    "    \n",
    "    C_ref = cupy_matmul_library(A_gpu, B_gpu)\n",
    "    if check_correctness(C_tiled, C_ref, tolerance=1e-3):\n",
    "        print(f\"{name} Validated\")\n",
    "        results[size] = time_tiled\n",
    "    else:\n",
    "        print(f\"{name} FAILED\")\n",
    "\n",
    "print(\"\\n=== Final Analysis ===\")\n",
    "best_tile = min(results, key=results.get)\n",
    "print(f\"Best Configuration: Tiled {best_tile}x{best_tile}\")\n",
    "print(f\"Speedup (Tiled vs Naive): {time_naive / results[best_tile]:.2f}x\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc8143",
   "metadata": {},
   "source": [
    "## Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "070d9cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final High-Standard Benchmark (Avg of 5 Runs)\n",
      "[Naive (16x16)] Avg Time: 39.6574 ms (±5.11 ms) | Runs: 5\n",
      "[Tiled (16x16)] Avg Time: 26.4581 ms (±0.09 ms) | Runs: 5\n",
      "[Tiled (32x32)] Avg Time: 21.7216 ms (±0.93 ms) | Runs: 5\n",
      "[CuPy Library] Avg Time: 3.4752 ms (±0.36 ms) | Runs: 10\n",
      "\n",
      "Final Results Summary:\n",
      "Method               | Time (ms)       | Speedup vs Naive    \n",
      "------------------------------------------------------------\n",
      "Naive (16x16)        | 39.6574         | 1.00x               \n",
      "Tiled (16x16)        | 26.4581         | 1.50                \n",
      "Tiled (32x32)        | 21.7216         | 1.83                \n",
      "CuPy Library         | 3.4752          | 11.41               \n"
     ]
    }
   ],
   "source": [
    "print(\"Final High-Standard Benchmark (Avg of 5 Runs)\")\n",
    "N = 2000\n",
    "A_host, B_host = generate_matrices(N)\n",
    "A_gpu, B_gpu = transfer_to_gpu(A_host, B_host)\n",
    "\n",
    "C_naive, time_naive = benchmark_function(\n",
    "    lambda: run_custom_kernel(A_gpu, B_gpu, N, block_size=(16,16)), \n",
    "    \"Naive (16x16)\",\n",
    "    n_iter=5\n",
    ")\n",
    "\n",
    "C_tiled_16, time_tiled_16 = benchmark_function(\n",
    "    lambda: run_tiled_kernel_dynamic(A_gpu, B_gpu, N, tile_size=16), \n",
    "    \"Tiled (16x16)\",\n",
    "    n_iter=5\n",
    ")\n",
    "\n",
    "C_tiled_32, time_tiled_32 = benchmark_function(\n",
    "    lambda: run_tiled_kernel_dynamic(A_gpu, B_gpu, N, tile_size=32), \n",
    "    \"Tiled (32x32)\",\n",
    "    n_iter=5\n",
    ")\n",
    "\n",
    "C_lib, time_lib = benchmark_function(\n",
    "    lambda: cupy_matmul_library(A_gpu, B_gpu), \n",
    "    \"CuPy Library\",\n",
    "    n_iter=10  \n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nFinal Results Summary:\")\n",
    "print(f\"{'Method':<20} | {'Time (ms)':<15} | {'Speedup vs Naive':<20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Naive (16x16)':<20} | {time_naive:<15.4f} | {'1.00x':<20}\")\n",
    "print(f\"{'Tiled (16x16)':<20} | {time_tiled_16:<15.4f} | {time_naive/time_tiled_16:<20.2f}\")\n",
    "print(f\"{'Tiled (32x32)':<20} | {time_tiled_32:<15.4f} | {time_naive/time_tiled_32:<20.2f}\")\n",
    "print(f\"{'CuPy Library':<20} | {time_lib:<15.4f} | {time_naive/time_lib:<20.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
